# -*- coding: utf-8 -*-
"""lstm.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/135rm8RzCuc4cWA6fuyNj8iWItAF-3kNe
"""

# Commented out IPython magic to ensure Python compatibility.
import googleapiclient.discovery
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline
from urllib.parse import urlparse, parse_qs

api_service_name = "youtube"
api_version = "v3"
DEVELOPER_KEY = "AIzaSyC1YUwsMt-dRKwU1WRJ9YUZZewj7eNjVZQ"
youtube = googleapiclient.discovery.build(api_service_name, api_version, developerKey=DEVELOPER_KEY)

video_link = input("Enter the YouTube Video Link: ")

parsed_url = urlparse(video_link)
video_id = parse_qs(parsed_url.query).get("v")
if video_id:
    video_id = video_id[0]

    request = youtube.commentThreads().list(
        part="snippet",
        videoId=video_id,
        maxResults=500
    )
    response = request.execute()

    comments = []
    for item in response['items']:
        comment = item['snippet']['topLevelComment']['snippet']
        comments.append([
            comment['authorDisplayName'],
            comment['publishedAt'],
            comment['updatedAt'],
            comment['likeCount'],
            comment['textDisplay']
        ])

    df = pd.DataFrame(comments, columns=['author', 'published_at', 'updated_at', 'like_count', 'text'])

    csv_filename = "youtube_comments.csv"
    df.to_csv(csv_filename, index=False)
else:
    print("Invalid YouTube video link.")

dataset = pd.read_csv('youtube_comments.csv')

dataset.drop(columns = 'author',axis = 1, inplace = True)
dataset.drop(columns = 'published_at',axis = 1, inplace = True)
dataset.drop(columns = 'updated_at',axis = 1, inplace = True)
dataset.drop(columns = 'like_count',axis = 1, inplace = True)

!pip install vadersentiment
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer  # Import the SentimentIntensityAnalyzer class
import nltk
nltk.download('vader_lexicon')
sentiments = SentimentIntensityAnalyzer()
dataset["Positive"] = [sentiments.polarity_scores(i)["pos"] for i in dataset["text"]]
dataset["Negative"] = [sentiments.polarity_scores(i)["neg"] for i in dataset["text"]]
dataset["Neutral"] = [sentiments.polarity_scores(i)["neu"] for i in dataset["text"]]
dataset['Compound'] = [sentiments.polarity_scores(i)["compound"] for i in dataset["text"]]
score = dataset["Compound"].values
sentiment = []
for i in score:
    if i >= 0.05 :
        sentiment.append('Positive')
    elif i <= -0.05 :
        sentiment.append('Negative')
    else:
        sentiment.append('Neutral')
dataset["Sentiment"] = sentiment
dataset.head()

data_new=dataset.drop(['Positive','Negative','Neutral','Compound'],axis=1)
data_new.head()

from sklearn.preprocessing import LabelEncoder
from sklearn.utils import resample
from sklearn.feature_extraction.text import CountVectorizer
from nltk.sentiment.vader import SentimentIntensityAnalyzer

from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from nltk.stem import PorterStemmer, LancasterStemmer
from nltk.stem.snowball import SnowballStemmer
from nltk.corpus import stopwords
from nltk.corpus import wordnet
import string
from string import punctuation

import nltk
import re
nltk.download('stopwords')
stop_words = stopwords.words('english')
porter_stemmer = PorterStemmer()
lancaster_stemmer = LancasterStemmer()
snowball_stemer = SnowballStemmer(language="english")
lzr = WordNetLemmatizer()

def text_processing(text):
      if not text.strip():
          return ''

      if all(ord(char) < 128 for char in text):
          stop_words = set(stopwords.words('english'))
          tokens = word_tokenize(text)
          text = ' '.join([word for word in tokens if word not in stop_words])
          return text

      text = text.lower()
      text = re.sub(r'\n', ' ', text)
      text = re.sub('[%s]' % re.escape(punctuation), "", text)
      text = re.sub("^a-zA-Z0-9$,.", "", text)
      text = re.sub(r'\s+', ' ', text, flags=re.I)

      try:
          lang = detect(text)
      except:
          lang = 'en'

      stop_words = set(stopwords.words('english'))
      tokens = word_tokenize(text)
      text = ' '.join([word for word in tokens if word not in stop_words])
      return text

import nltk
nltk.download('punkt')
nltk.download('omw-1.4')
nltk.download('wordnet')
data_copy = data_new.copy()
data_copy.Comment = data_copy.text.apply(lambda text: text_processing(text))

score = dataset["Compound"].values
sentiment = ['Positive' if s >= 0.05 else 'Negative' if s <= -0.05 else 'Neutral' for s in score]

dataset["Sentiment"] = sentiment

    # Text Preprocessing
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

dataset['text'] = dataset['text'].apply(preprocess_text)

tokenizer = Tokenizer(num_words=5000, oov_token='<OOV>')
tokenizer.fit_on_texts(dataset['text'])
X = tokenizer.texts_to_sequences(dataset['text'])
X = pad_sequences(X, maxlen=100)

    # Encoding sentiment labels
label_encoder = LabelEncoder()
dataset['Sentiment'] = label_encoder.fit_transform(dataset['Sentiment'])

y = dataset['Sentiment']

    # Splitting the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # LSTM Model
model = Sequential()
model.add(Embedding(5000, 128))  # Remove input_length parameter
model.add(SpatialDropout1D(0.4))
model.add(LSTM(128, dropout=0.5, recurrent_dropout=0.5))
model.add(Dense(3, activation='softmax'))
model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
history = model.fit(X_train, y_train, epochs=100, batch_size=64, validation_data=(X_test, y_test), verbose=2)

_, accuracy = model.evaluate(X_test, y_test)
print("Accuracy:", accuracy)
average_score = np.mean(score)
if average_score > 0:
    print("Whooooo! The video is perceived positively.")
elif average_score < 0:
    print("Ooops! The video is perceived negatively.")
else:
    print("Hmmmm! The video is perceived as Neutral.")

from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix
y_pred = model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)
f1 = f1_score(y_test, y_pred_classes, average='weighted')
precision = precision_score(y_test, y_pred_classes, average='weighted')
recall = recall_score(y_test, y_pred_classes, average='weighted')
conf_matrix = confusion_matrix(y_test, y_pred_classes)

print("F1 Score:", f1)
print("Precision:", precision)
print("Recall:", recall)
print("Confusion Matrix:")
print(conf_matrix)

from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix
import matplotlib.pyplot as plt
import numpy as np
import itertools

# Calculate precision, recall, f1-score, and accuracy
precision = precision_score(y_test, y_pred_classes, average='weighted')
recall = recall_score(y_test, y_pred_classes, average='weighted')
f1 = f1_score(y_test, y_pred_classes, average='weighted')
accuracy = accuracy_score(y_test, y_pred_classes)

# Print the metrics
print("Precision:", precision)
print("Recall:", recall)
print("F1 Score:", f1)
print("Accuracy:", accuracy)

# Calculate and plot confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred_classes)

plt.figure(figsize=(8, 6))
plt.imshow(conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)
plt.title('Confusion Matrix')
plt.colorbar()
classes = np.unique(y_test)
tick_marks = np.arange(len(classes))
plt.xticks(tick_marks, classes, rotation=45)
plt.yticks(tick_marks, classes)

fmt = 'd'
thresh = conf_matrix.max() / 2.
for i, j in itertools.product(range(conf_matrix.shape[0]), range(conf_matrix.shape[1])):
    plt.text(j, i, format(conf_matrix[i, j], fmt),
             horizontalalignment="center",
             color="white" if conf_matrix[i, j] > thresh else "black")

plt.tight_layout()
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.show()

# Convert probabilities to class labels
y_pred_classes = np.argmax(y_pred, axis=1)

# Calculate precision, recall, f1-score, and accuracy
precision = precision_score(y_test, y_pred_classes, average='weighted')
recall = recall_score(y_test, y_pred_classes, average='weighted')
f1 = f1_score(y_test, y_pred_classes, average='weighted')
accuracy = accuracy_score(y_test, y_pred_classes)

import numpy as np

print("y_test shape:", y_test.shape)
print("y_pred shape:", y_pred.shape)

print("y_test unique values:", np.unique(y_test))
print("y_pred unique values:", np.unique(y_pred))

precision = precision_score(y_test, y_pred_classes, average='weighted')
recall = recall_score(y_test, y_pred_classes, average='weighted')
f1 = f1_score(y_test, y_pred_classes, average='weighted')
accuracy = accuracy_score(y_test, y_pred_classes)

# Plotting
metrics = ['Precision', 'Recall', 'F1 Score', 'Accuracy']
values = [precision, recall, f1, accuracy]

plt.figure(figsize=(8, 6))
plt.plot(metrics, values, marker='o', color='blue', linestyle='-')
plt.title('Metrics Comparison')
plt.xlabel('Metrics')
plt.ylabel('Score')
plt.ylim(0, 1)  # Limiting y-axis from 0 to 1
plt.grid(True)
plt.show()